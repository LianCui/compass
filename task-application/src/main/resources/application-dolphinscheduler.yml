spring:
  datasource:
    url: jdbc:mysql://localhost:33066/diagnose_data?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai
    username: root
    password: root
    druid:
      initial-size: 5
      min-idle: 10
      max-active: 20
  kafka:
    bootstrap-servers: "localhost:9095"
    topics: "task-instance"
    consumer:
      group-id: "cp-task-application"
      auto-offset-reset: "earliest"
      max-poll-interval-ms: 300000
  redis:
    cluster:
      nodes: localhost:6379
      max-redirects: 3
    lettuce:
      pool:
        max-active: 32
        max-idle: 16
        min-idle: 8

custom:
  delayedTask:
    enable: true
    queue: "{lua}:task:application"
    processing: "{lua}:task-processing"
    delayedSeconds: 5
    tryTimes: 20
  # 从上到下串行执行解析到任务的applicationId
  rules:
    - logPathDep: # 日志依赖查询
        query: "select CASE WHEN end_time IS NOT NULL THEN DATE_ADD(end_time, INTERVAL 1 second) ELSE start_time END as end_time,log_path from t_ds_task_instance where id=${id}"     # 查询, id 是 task-instance表的id
      logPathJoins: # 组成日志路径
        # end_time: 2023-02-18 01:43:11
        # log_path: ../logs/6354680786144_1/3/4.log
        - { "column": "", "data": "hdfs://log-hdfs:8020/flume/dolphinscheduler" } # 配置存储调度日志的hdfs根目录，log-hdfs即nameservice需要根据实际集群修改
        - { "column": "end_time", "regex": "^.*(?<date>\\d{4}-\\d{2}-\\d{2}).+$", "name": "date" }
        - { "column": "log_path", "regex": "^.*logs/(?<logpath>.*)$", "name": "logpath" }
      extractLog: # 根据组装的日志路径解析日志
        regex: "^.*Submitted application (?<applicationId>application_[0-9]+_[0-9]+).*$"     # 匹配规则
        name: "applicationId"      # 匹配文本名，最后必须有applicationId